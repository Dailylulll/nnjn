{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f8d0d9577bfce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notes:\n",
    "1. projects must be launched from iris working directory\n",
    "9. integrate tfds into loops\n",
    "13. find more stats for regression analysis\n",
    "14. learn how to use compilers, hooks and registers to speed up processing\n",
    "15. use sampler to increase information quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pprint\n",
    "# jupyter notebook magic\n",
    "\n",
    "%load_ext tensorboard"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c454c4050f7aa6d"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T20:26:27.514973700Z",
     "start_time": "2024-01-26T20:26:21.163652800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "import torch.utils.tensorboard as tb\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "import general_torch as gt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12800aedc886b754",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init exp from scratch\n",
    "# each dictionary represents an individual experiment  # todo important point here\n",
    "# todo use itertools to generate dictionaryies for each\n",
    "\n",
    "exp = {}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float\n",
    "exp['model'] = 'model0'\n",
    "exp['model_init_file'] = 'model0_0.pth'\n",
    "exp['optim'] = 'sgd'\n",
    "exp['optim_dict'] = {'lr': 0.001, 'momentum': 0.4}  # if a value is part of the experiment, leave it empty and init it where appropriate\n",
    "exp['error_f'] = 'mse'\n",
    "exp['stop'] = 'epoch'\n",
    "exp['stop_val'] = None\n",
    "exp['max_epoch'] = 100\n",
    "exp['data_file'] = 'iris_tensor_dict.pt'   # put the tensors into one dict and this will be the file extension!\n",
    "exp['dl_dict'] = {'batch_size': 1, 'shuffle': True}  # 'data_batch_size': None, 'random_batch': False,\n",
    "exp['model_type'] = 'classification'  # regression, class\n",
    "exp['seed'] = 42\n",
    "exp['independent_var'] = 'lr'  \n",
    "exp['hparams'] = {'lr': 0.001, 'optimizer': 'sgd'}\n",
    "exp['device'] = device\n",
    "exp['data_type'] = dtype\n",
    "exp['exp_file'] = f'experiments/troubleshoot_dump/run1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# init profiler log paths\n",
    "cwd = os.getcwd()\n",
    "exp_dir = os.path.join(cwd, exp['exp_file'])\n",
    "train_log= os.path.join(exp_dir, 'train')\n",
    "test_log = os.path.join(exp_dir, 'test')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45ac51afdf8cc0b1"
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1833066d0443e08d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T03:50:47.713387900Z",
     "start_time": "2024-01-23T03:50:47.693613800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# raw data loader, change device, dtype and if needed shape\n",
    "# this needs to change, take exp file, load the dict, use dict to distribute to tensors\n",
    "\n",
    "file_tensor = os.path.join(cwd, f'data/{exp[\"data_file\"]}')\n",
    "\n",
    "dict_tensor = torch.load(file_tensor)\n",
    "\n",
    "train_data = dict_tensor['train_data'].to(exp['device']).to(exp['data_type'])\n",
    "train_label = dict_tensor['train_label'].to(exp['device']).to(exp['data_type'])\n",
    "\n",
    "valid_data = dict_tensor['valid_data'].to(exp['device']).to(exp['data_type'])\n",
    "valid_label = dict_tensor['valid_label'].to(exp['device']).to(exp['data_type'])\n",
    "\n",
    "test_data = dict_tensor['test_data'].to(exp['device']).to(exp['data_type'])\n",
    "test_label = dict_tensor['test_label'].to(exp['device']).to(exp['data_type'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "af8aeffedfec4b09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T04:04:04.624425200Z",
     "start_time": "2024-01-23T04:04:04.589489700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init data set\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, train, label):\n",
    "        self.data = train\n",
    "        self.label = label\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "        \n",
    "train_set = Data(train_data, train_label)\n",
    "valid_set = Data(valid_data, valid_label)\n",
    "test_set = Data(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4dc291a585f6e1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# always start by resetting seeds before initializing anything\n",
    "gt.init_seeds(exp['seed'], exp['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "da455aa1ece1d10f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T04:04:05.419687500Z",
     "start_time": "2024-01-23T04:04:05.394288900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prep dataloader, use kwargs to init\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_set, **exp['dl_dict'])  \n",
    "valid_loader = DataLoader(valid_set, **exp['dl_dict'])\n",
    "test_loader = DataLoader(valid_set, **exp['dl_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "932a14f591825c47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T04:04:07.721650600Z",
     "start_time": "2024-01-23T04:04:07.690276300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model0_2(\n",
       "  (lin1): Linear(in_features=1, out_features=50, bias=True)\n",
       "  (lin2): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (lin3): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build model / load state dict / load whole model / explicitly init param weights\n",
    "import models\n",
    "\n",
    "model = models.get_model(exp['model'])()  \n",
    "model.load_state_dict(models.load_trained_model(exp['model_init_file']))  \n",
    "model.to(exp['data_type'])\n",
    "model.to(exp['device'])\n",
    "epochs = exp['max_epoch']\n",
    "optim = gt.get_optim(exp['optim'])(model.parameters(), **exp['optim_dict'])\n",
    "error = gt.get_error(exp['error_f'])()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check for runtime errors / init predictions should be part of data collection loop\n",
    "'''for i in model.parameters():\n",
    "    print(i.dtype)\n",
    "    break\n",
    "print(test_data.dtype)'''\n",
    "\n",
    "with torch.inference_mode():  \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        model(test_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b0ac55e6ed340eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# init tensorboard\n",
    "# writer needs independent var, trial num, model\n",
    "# write model, params, flops, train time, inference time, memory usage, profiler, can be used here\n",
    "# write trial num, model and independent var\n",
    "\n",
    "writer = tb.writer.SummaryWriter(log_dir=exp_dir)\n",
    "writer.add_hparams(exp['hparams'], metric_dict={})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94f281e0afc4681d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501937da37b0069",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init test predictions, to test overall improvement\n",
    "\n",
    "with torch.inference_mode():  \n",
    "    pred_list = []\n",
    "    target_list = []\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        preds = model(test_data)\n",
    "        loss = error(preds, target)\n",
    "        pred_list.append(preds)\n",
    "        target_list.append(target)\n",
    "        total += loss\n",
    "    avg_error = total / len(test_data)\n",
    "    gt.write_test_info(exp, writer, target_list, pred_list, avg_error, 'test')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# modify hyperparams\n",
    "\n",
    "modify_hyper = False\n",
    "if modify_hyper:\n",
    "    epochs = 10\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.4)\n",
    "    error = torch.nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5633f99d4b5d6eae"
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "dcf8fa03522939ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T04:04:15.150574500Z",
     "start_time": "2024-01-23T04:04:13.958268Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need seperate data collection and examination loops here, also switch to dataloader, use skinny flag to fork collection\n",
    "\n",
    "# for train and valid, log every epoch\n",
    "\n",
    "'''\n",
    "test  - accuracy, precision, f1, recall, error, gradients and weights\n",
    "valid - accuracy, precision, f1, recall, error\n",
    "\n",
    "there are two\n",
    "online stats are collected after a complete epoch\n",
    "batch stats are after each batch\n",
    "mini_batch, after whole batch\n",
    "\n",
    "'''\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], \n",
    "             schedule=torch.profiler.schedule(wait=2, warmup=2, active=1),\n",
    "             on_trace_ready=torch.profiler.tensorboard_trace_handler(os.path.join(train_log, 'profile')),\n",
    "             record_shapes=True,\n",
    "             with_stack=True) as prof:\n",
    "\n",
    "    error_last = None\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        target_list = [] \n",
    "        pred_list = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            pred = model(data)\n",
    "            loss = error(pred, target)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total += loss\n",
    "            target_list.append(target)\n",
    "            pred_list.append(pred)\n",
    "            prof.step()\n",
    "        avg_error = total / len(train_data)\n",
    "        gt.write_train_info(exp, writer, model, target_list, pred_list, avg_error, 'train')\n",
    "\n",
    "        model.eval()\n",
    "        error_new = None\n",
    "        total = 0\n",
    "        target_list = []\n",
    "        pred_list = []\n",
    "        for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "            pred = model(data)\n",
    "            error_new = error(pred, target)\n",
    "            total += error_new\n",
    "            target_list.append(target)\n",
    "            pred_list.append(pred)\n",
    "        avg_error = total / len(valid_data)\n",
    "        gt.write_train_info(exp, writer, model, target_list, pred_list, avg_error, 'valid')\n",
    "        if exp['stop'] == 'error_change':\n",
    "            if error_last is not None and error_new / error_last < exp['stop_value']:\n",
    "                break\n",
    "            else:\n",
    "                error_last = error_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "463a10d0f577adbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T04:04:16.532526600Z",
     "start_time": "2024-01-23T04:04:16.506422900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run final inference\n",
    "\n",
    "# init test predictions, to test overall improvement\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], \n",
    "             schedule=torch.profiler.schedule(wait=2, warmup=2, active=3),\n",
    "             on_trace_ready=torch.profiler.tensorboard_trace_handler(os.path.join(test_log, 'profile')),\n",
    "             record_shapes=True,\n",
    "             with_stack=True) as prof:\n",
    "\n",
    "    with torch.inference_mode():  \n",
    "        pred_list = []\n",
    "        target_list = []\n",
    "        total = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            preds = model(test_data)\n",
    "            loss = error(preds, target)\n",
    "            pred_list.append(preds)\n",
    "            target_list.append(target)\n",
    "            total += loss\n",
    "            prof.step()\n",
    "        avg_error = total / len(test_data)\n",
    "        gt.write_test_info(exp, writer, target_list, pred_list, avg_error, 'test')\n",
    "    \n",
    "writer.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157fe38eb6c9e56d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir='experiments/troubleshoot_dump/run1'  # this has to be set manually as this is a terminal \n",
    "\n",
    "'''if skinny_run:\n",
    "    pass\n",
    "    # skiny eval with no data collection, but still with tensor board\n",
    "else:\n",
    "    pass\n",
    "    # eval model -- tensorboard, events, profiler, take predictions overtime and create a seaborn graph, build a suite!'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821e46e8c04b833",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mkdown for notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save trial folder need pred list, model and above mkdown, to the exp['exp_dir']\n",
    "\n",
    "import pickle\n",
    "\n",
    "file_dir = {'preds': pred_list, 'lables': target_list, 'exp': exp}\n",
    "filename = os.path.join(exp_dir, 'pred_dir.pkl')\n",
    "\n",
    "with open(filename, \"wb\") as file:\n",
    "    pickle.dump(file_dir, file)\n",
    "    \n",
    "filename = os.path.join(exp_dir, 'info.txt')\n",
    "\n",
    "with open(filename, 'w') as file:\n",
    "    pprint.pprint(exp, file)\n",
    "    \n",
    "filename = os.path.join(exp_dir, f'{exp[\"model\"]}.pth')\n",
    "torch.save(model.state_dict(), filename)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2a4db72db148222"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this ends the trial loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f9d9a4658d02653"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
